
== Abstract

When defining the integration of Helm packages in Shipper, we've opted to have a very simple and constrained design, established by the following rules: *one and only one* deployment present in the chart, and *one and only one service* with a stable name and a specific label to act as load balancing service. This design has served us well until now, but it has a clear drawback: it requires that special Helm packages *must* be created in order to be compatible with Shipper.

This RFC proposes formally specifying a compatibility layer in Shipper to make it possible to use publicly available charts with it.

== Rationale

Shipper's contract with Helm charts specify that a chart needs to contain *one and only one* deployment manifest and *one and only one* load balancing service manifest. Having multiple Kubernetes services is useful for, for example, enable access to pods belonging only to the new version of an image *before* starting a release process.

To identify this load balancing service manifest, once it is installed on a Kubernetes cluster, a label on the specific load balancing service is used (`shipper-lb: production` more specifically). This choice made any existing chart on the wild incompatible with Shipper; this has a very important and awkward side-effect: it is very inconvenient to even try Shipper out without having an entire environment set prior testing it (such as a Helm repository and a chart crafted specially to work with Shipper).

Being able to easily use off-the-shelf charts can bring the following benefits:

- Less documentation to be written regarding setting a whole environment to just try Shipper on a Minikube or Docker for Desktop Kubernetes installation.
- Shipper becomes much more attractive to a much wider audience, increasing the changes of adoption by the ecosystem.

== Specification

To overcome that limitation, we want to introduce the field `.spec.template.loadBalancer` field in the Application CRD; this field will be used to propagate relevant load balancing configuration to Shipper.

== Approaches

There are at least two ways of approaching this problem:

- Introduce specific keys under `.spec.template.loadBalancer`; or
- Make `.spec.template.loadBalancer` contain generic, open ended load balancing back-end configurations based on GVKs.

IMPORTANT: This RFC *will not* touch any design related to supporting multiple load balancing back-end implementations, but mention this at several points because this configuration should be propagated to any existing implementation.

=== Configuration specific to each load balancing back-end implementation

The first option is a very explicit and type safe approach, on the expense of modifying the type backing this field every time a new load balancing back-end implementation is created. This also poses a challenge on designing support and integration with any kind of proprietary Service Mesh control plane implementation, specially when building Shipper. Another point is that any extension on the aforementioned type will be a backwards incompatible change, requiring a new resource definition version and a migration process.

The following listing shows an example of the first approach:

.Specific load balancing back-end configuration example.
[source,yaml]
----
spec:
  template:
    loadBalancer:     # <1>
      kubernetes:     # <2>
        name: my-app  # <3>
        selector: {}  # <4>
      istio:
        ...       # Relevant Istio configuration here
----

<1> `.spec.template.loadBalancer` represents the load balancer configuration for the application, one field per known implementation.
<2> `.spec.template.loadBalancer.kubernetes` holds the configuration for vanilla Kubernetes traffic shifting implementation.
<3> Name of the Kubernetes service manifest that should be used as load balancing service.
<4> Selectors that should be used to overwrite the load balancing service selector.

=== Generic and open load balancing configuration

The second approach loses the type safety but doesn't require any changes to the backing type: the configuration would be a map of type `map[string]interface{}`, where the string key is a representation of a Kubernetes object's GVK. This means that load balancing back-end implementations would use the configuration related to the GVK they're interested; for example, the current implementation uses Kubernetes' services as load balancing interfaces, so it'd contain a configuration for the `v1.Service` GVK representation.

The following listing exemplifies the second approach:

.Specifying per GVK configuration.
[source,yaml]
----
spec:
  template:
    loadBalancer:   <1>
      "v1.Service": <2>
        name: my-app
        selector: {}
      "networking.istio.io/v1alpha3.VirtualService":
        ...       # Relevant Istio configuration here
----

<1> `.spec.template.loadBalancer` is backed by a `map[string]interface{}`.
<2> The key format is `apiVersion.Kind`. In the case of Kubernetes core resource definitions, `apiVersion` is only `v1`, thus `v1.Service`. It is important to mention the `loadBalancer` field must be *explicitly encoded* as a string; for example `v1.Service` is different than `"v1.Service"` due the fact of `v1.Service` as a key is a shortcut for adding sub-objects when composing YAML documents.

When Shipper is executing a load balancing back-end that uses 'v1.Service' objects to perform traffic shifting, the given configuration will be available to the implementation.

A nice property of this approach is that any load balancing back-end implementation might be added and registered when building Shipper, and be further enabled through Shipper command line options at runtime without requiring additional modifications.

== Implementation

The implementation described on this section is the second presented, "Generic and open load balancing configuration". This choice was made by the writer because:

- It is simple. A root object is offered with the type `map[string]interface{}`, so back-end implementers are free to choose the most appropriate layout for their configuration from that point on. Additionally, there's no reason for back-end implementers to not try to coerce from this liberal map to another data-structure of their choice when consuming it.
- Is future proof. As long as keys don't overlap, their meaning can be changed without much effort if GVKs are proven problematic.

=== High level activities to be performed

- Add a new field in `v1.ReleaseEnvironment` called `LoadBalancer` of type `[]LoadBalancer`
+
[source,go]
----
type ReleaseEnvironment struct {
        // ...
        LoadBalancer []LoadBalancer `json:"loadBalancer,omitempty" <1>
}
----
+
<1> The absence of the `loadBalancer` key might be useful during migration to indicate that the original behavior (searching and ensuring that a valid service with the appropriate label exists) is the intended behavior.
- Add a new type `v1.LoadBalancer`, defined as:
+
[source,go]
----
type LoadBalancer map[string]interface{}
----
+
This allows the following configuration to be stored and later on consumed by the Installation and Traffic Controllers:
+
[source,yaml]
----
spec:
  template:
    loadBalancer:
      "v1.Service":
        name: my-app
        selector: {}
----
- Modify Installation Controller to conditionally patch the service before installation on application clusters. The `selector` field can be either `nil`, meaning that no operation will be carried regarding the service's selector, or be a dictionary of type `map[string]string` which will be used to replace the service's `selector` field. (*Note:* do we need to perform any check or patching in the deployment as well?)
- Modify Traffic Controller business logic to use the configuration to find the service that should be used to perform traffic shifting. The `name` field should contain the service name in the application's namespace.

IMPORTANT: Types names and definitions are subject to discussion, just presented like this to have a head start.
