
== Abstract

When defining the integration of Helm packages in Shipper, we've opted to have a very simple and constrained design, established by the following rules: *one and only one* deployment present in the chart, and *one and only one service* with a specific label. This design has served us well until now, but it has a clear drawback: it requires that special Helm packages *must* be created in order to be compatible with Shipper.

This RFC proposes formally specifying a compatibility layer in Shipper to make it possible to use publicly available charts with it.

== Rationale

Shipper's contract with Helm charts specify that a chart needs to contain *one and only one* deployment manifest and *one and only one* load balancing service manifest. Having multiple Kubernetes service is useful for, for example, enable access to pods belonging only to the new version of an image *before* starting a release process.

To identify this load balancing service manifest, once it is installed on a Kubernetes cluster, a label on the specific load balancing service is used (`shipper-lb: production` more specifically). This choice made any existing chart on the wild incompatible with Shipper.

== Specification

To overcome that limitation, we want to introduce the field `.spec.template.loadBalancer` field in the Application CRD; this field will be used to propagate relevant load balancing configuration to Shipper.

== Approaches

There are at least two ways of approaching this problem:

- Introduce specific keys under `.spec.template.loadBalancer`; or
- Make `.spec.template.loadBalancer` contain generic, open ended load balancing back-end configurations based on GVKs.

IMPORTANT: This RFC *will not* touch any design related to supporting multiple load balancing back-end implementations, but mention this at several points because this configuration should be propagated to any existing implementation.

=== Configuration specific to each load balancing back-end implementation

The first option is a very explicit and type safe approach, on the expense of modifying the type backing this field every time a new load balancing back-end implementation is created. This also poses a challenge on designing support and integration with any kind of proprietary Service Mesh control plane implementation, specially when building Shipper. Another point is that any extension on the aforementioned type will be a backwards incompatible change, requiring a new resource definition version and a migration process.

The following listing shows an example of the first approach:

.Specific load balancing back-end configuration example.
[source,yaml]
----
spec:
  template:
    loadBalancer:     # <1>
      kubernetes:     # <2>
        name: my-app  # <3>
        selector: {}  # <4>
      istio:
        ...       # Relevant Istio configuration here
----

<1> `.spec.loadBalancer` represents the load balancer configuration for the application, one field per known implementation.
<2> `.spec.loadBalancer.kubernetes` holds the configuration for vanilla Kubernetes traffic shifting implementation.
<3> Name of the Kubernetes service manifest that should be used as load balancing service.
<4> Selectors that should be used to overwrite the load balancing service selector.

=== Generic and open load balancing configuration

The second approach loses the type safety but doesn't require any changes to the backing type: the configuration would be a list of objects containing the `gvk` and `options` fields. This means that load balancing back-end implementations would use the configuration related to the GVK they're interested; for example, the current implementation uses Kubernetes' services as load balancing interfaces, so it'd contain a configuration for the `v1.Service` gvk.

The following listing exemplifies the second approach:

.Specifying per GVK configuration.
[source,yaml]
----
spec:
  template:
    loadBalancer:   <1>
      "v1.Service": <2>
        name: my-app
        selector: {}
      "networking.istio.io/v1alpha3.VirtualService":
        ...       # Relevant Istio configuration here
----

<1> `.spec.template.loadBalancer` is backed by a `map[string]interface{}`.
<2> The key format is `apiVersion.Kind`. In the case of Kubernetes core resource definitions, `apiVersion` is only `v1`, thus `v1.Service`. It is important to mention the `loadBalancer` field must be *explicitly encoded* as a string; for example `v1.Service` is different than `"v1.Service"` due the fact of `v1.Service` as a key is a shortcut for adding sub-objects when composing YAML documents.

When Shipper is executing a load balancing back-end that uses 'v1.Service' objects to perform traffic shifting, the given configuration will be available to the implementation.

A nice property of this approach is that any load balancing back-end implementation might be added and registered when building Shipper, and be further enabled through Shipper command line options at runtime without requiring additional modifications.

== TODO

- Specification of `loadBalancer`
  * One key per known back-end implementation
- Specification of `loadBalancer.kubernetes`
- Implementation